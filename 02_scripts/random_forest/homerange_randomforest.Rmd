---
title: "Home Range Random Forest"
author: "Allie Caughman"
date: "1/10/2022"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(here)
library(tidymodels)
library(ranger)
library(patchwork)
library(DALEX)
library(kableExtra)

set.seed(320)
``` 

```{r, include=FALSE}
weighted.geomean <- function(x, w) {
  return(prod(x^w)^(1 / sum(w))) # calculates weighted geometric mean
}
```

```{r, include=FALSE}
## Read in all data
full_data <- read_csv(here("01_data", "02_processed_data", "homerange_data.csv")) %>% # load data
  mutate(movement_keyword = as.factor(movement_keyword)) %>% # turn categorical variables into factors
  mutate(family = as.factor(family)) %>% # turn categorical variables into factors
  select(species, family, homerange, n_ind, geog_range, movement_keyword, demers_pelag, length, common_name, kfin, r_fin, diet_troph3, Class, iucn_category) %>% # get relevant columns
  rename(hr = homerange) %>% # rename homerange
  filter(!is.na(r_fin)) %>% # remove columns with NA in predictors
  filter(!is.na(kfin)) %>% # remove columns with NA in predictors
  filter(!is.na(length)) %>% # remove columns with NA in predictors
  filter(!is.na(diet_troph3)) %>% # remove columns with NA in predictors
  filter(!is.na(movement_keyword)) %>% # remove columns with NA in predictors
  filter(!is.na(geog_range)) %>% # remove columns with NA in predictors
  distinct() %>% 
  select(-r_fin, -kfin, -geog_range)

n_ind <- ifelse(!is.na(full_data$n_ind), full_data$n_ind, 1) # replace NA with 1
full_data$n_ind <- n_ind # add replaced column back to dataframe

# filter data to get rows with home range values
num <- full_data %>%
  group_by(species) %>% # group by species
  summarize(count = n()) %>% # get species counts
  filter(count > 1) # remove species with only one instance

ready_data <- full_data %>%
  filter(!(species %in% num$species)) %>% # get species with only 1 instance
  filter(!is.na(hr)) %>% # remove values where home range does not exist)
  select(species, hr) # select only species and home range

mean_calc <- full_data %>%
  filter(species %in% num$species) %>% # get species with more than 1 instance
  filter(!is.na(hr)) %>% # remove values where home range does not exist
  filter(species != "Centropomus undecimalis") %>% # remove species that geomean = infinity
  group_by(species) %>% # group by species
  summarize(hr = weighted.geomean(hr, n_ind)) # calculated weighted geometric mean

centropomus_undecimalis <- full_data %>%
  filter(species == "Centropomus undecimalis") %>% # calculate mean for species that geomean = infinity. Samples size for 2 occurrences are equal
  group_by(species) %>%
  summarise(hr = mean(hr))

hr_averages <- full_join(ready_data, mean_calc) # combine to get full observed home range data set

hr_averages <- full_join(hr_averages, centropomus_undecimalis) # combine to get full observed home range data set

# get list of species without home range values to be predicted
merge_data <- full_data %>%
  distinct(species, .keep_all = TRUE) %>% # keep one instance of each species
  select(-hr) # remove HR column

rf_data <- left_join(hr_averages, merge_data) # join averages with corresponding predictor data

predict_data <- read_csv(here::here("01_data", "02_processed_data", "pisco_rf_data.csv"))
```


```{r, include=FALSE}
rf_split <- initial_split(rf_data, prop = .7, strata = family) # 75/25% split for training/testing

train <- training(rf_split) # make training split
test <- testing(rf_split) # make testing split

cv_folds <- vfold_cv(rf_data, repeats = 5, strata = family, v = 10) # folds for cross validation
```

### Overview

This model seeks to predict home range values for an array of fish species using 67 home range values found from the literature. Data were split into a testing and a training set and run through a random forest model using the ranger engine in R. The model is as follows:

home range ~ growth rate (r) + carrying capacity (K) + movement keyword + fish length + trophic level + geographic range

```{r, include=FALSE}
rf_recipe <- recipe(hr ~ length + diet_troph3 + movement_keyword, data = train) %>% # create prepossessing recipe
  step_normalize(all_numeric_predictors()) %>% # normalize all numeric variables
  step_log(all_outcomes(), skip = TRUE) # log transform outcome variable
```

```{r, include = FALSE}
rf_ranger <- rand_forest(trees = tune(), mtry = tune(), min_n = tune(), mode = "regression") %>% # set to tune all parameters with cross validation
  set_engine("ranger", verbose = TRUE) # set ranger regression engine
```

```{r, include = FALSE}
rf_wflw <- workflow() %>%
  add_model(rf_ranger) %>%
  add_recipe(rf_recipe) # create workflow with ranger model and preprocessed rf model
```

### Cross Validation

Cross validation will be used to choose the best hyperparameters for use in the model: 

* Number of trees
* Number of predictors in each sample
* Minimum data points in each node required for node split

```{r,include=FALSE}
rf_tune <- rf_wflw %>% # use cross validation to tune parameters
  tune_grid(
    resamples = cv_folds, # add cv folds
    grid = 10
  ) # use grid of parameters to test
```

Below displays the results of cross validation tuning

```{r}
autoplot(rf_tune) + # plot results of tuning
  theme_bw() # change theme
```

```{r}
select_best(rf_tune, metric = "rsq", n = 1) # get the best model
```

```{r, include=FALSE}
rf_final <- finalize_workflow(rf_wflw, select_best(rf_tune, metric = "rsq")) # finalize the workflow with the best model
```

```{r, include=FALSE}
rf_fit <- fit(rf_final, train) # fit the data to the training data
```

```{r, include=FALSE}
train_predict <- predict(object = rf_fit, new_data = train) %>% # predict the training set
  bind_cols(train) %>% # bind training set column to prediction
  mutate(pred = exp(.pred)) %>% # back transform predictions
  mutate(log_hr = log(hr)) # get natural log of hr

test_predict <- predict(object = rf_fit, new_data = test) %>% # predict the training set
  bind_cols(test) %>% # bind prediction to testing data columns
  mutate(pred = exp(.pred)) %>% # back transform predictions
  mutate(log_hr = log(hr)) %>% # get natural log of hr
  mutate(common_name = test$common_name) %>% # add common names back
  mutate(species = test$species) %>% # add species name back
  mutate(family = test$family) # add families back

predict_full <- full_join(rf_data, predict_data) # combine predict data with training data

prediction <- predict(object = rf_fit, new_data = predict_full) %>% # predict the training set
  bind_cols(predict_full) %>% # bind prediction to full data
  mutate(pred = exp(.pred)) %>% # back transform predictions
  select(-.pred) %>% # remove unneeded column
  rename(observed_homerange = hr) %>% # rename columns
  rename(predicted_homerange = pred) # rename columns

# write_csv(prediction, here::here("01_data", "02_processed_data", "pisco_rf_hr_predict.csv"))
```

Numeric predictor variables were normalized and and the home range values were log-transformed so that the large values did not over influence the results of the model. Root mean square error (RMSE) and $R^2$ were calculated on the test set to evaluate the model performance. Finally, the model was use to predict the home ranges for 600 additional species with unknown home range values.

```{r, include = FALSE}
train_metrics <- train_predict %>%
  metrics(log_hr, .pred) # get testing data metrics

test_metrics <- test_predict %>%
  metrics(log_hr, .pred) # get testing data metrics

# write_csv(test_metrics, here::here("results", "hr_rf_metrics.csv"))
```

### Metrics

First we calculated the testing and training RMSE and $R^2$ values. 
The training RSME was `r round(train_metrics$.estimate[1],2)` and the $R^2$ was `r round(train_metrics$.estimate[2],2)`.

Then cross validation was then preformed to get an average RMSE across folds

```{r, include = FALSE}
cv2 <- rf_final %>%
  fit_resamples(cv_folds) # fit the folds to the final model
```

The results of cross validation are:

```{r}
collect_metrics(cv2)
```

The testing metrics are:

```{r, include=FALSE}
metric_table <- test_metrics %>% # create nice table of metrics
  as.data.frame() %>%
  kbl(digits = 4, col.names = c("Metric", "Estimator", "Estimate")) %>%
  kable_minimal()
```

```{r}
metric_table
```

A low RMSE indicates a good fit of the model to the data.

### Diagonstics

#### Test accuracy of new prediction

Here are the predictions versus the actual values for the log-transformed model.

```{r, include=FALSE}
scatter_plot <- ggplot(test_predict, aes(x = log_hr, y = .pred)) + # plot ln of real versus ln of predicted
  geom_point() +
  geom_smooth(method = "lm") + # add trend line
  theme_bw() +
  labs(x = expression(ln(observed ~ home ~ range ~ (km^{
    "2"
  }))), y = expression(ln(predicted ~ home ~ range ~ (km^{
    "2"
  })))) +
  theme(text = element_text(size = 10))
```

```{r}
scatter_plot
```


```{r, include = FALSE}
lm <- lm(.pred ~ PLD, data = test_predict) # linear regression of ln of real versus ln of predicted
```

```{r}
summary(lm) # get summary of the regression
```


```{r, include = FALSE}
back <- ggplot(test_predict, aes(hr, pred)) + # plot real versus predicted back transformed
  geom_point() +
  geom_smooth(method = "lm") + # get trend line
  theme_bw() +
  labs(x = "Observed Home Range km^2", y = "Predicted Home Range km^2")

back

# ggsave(back, file = paste0("observed_vs_predicted_ln_back.pdf"), path = here::here("figs"))
```

```{r, include=FALSE}
lm_t <- lm(pred ~ hr, data = test_predict) # linear regression real versus predicted back transformed
summary(lm_t) # get summary of back tranformed regression
```

#### Variable Imporance

```{r, include = FALSE}
hrs_test <- test$hr # extract HR for working with DALEX
test_vip <- test %>% # extract predictors for working with DALEX
  select(-Class, -common_name, -demers_pelag, -family, -iucn_category, -n_ind, -species, -hr)

expl <- explain(rf_fit, data = test_vip, y = hrs_test, predict_function = function(x, y) {
  predict(x, new_data = y) %>% pull(.pred)
}) # build DALEX explainer of the random forest model predicting test data

var_imp <- variable_importance(expl) %>%
  mutate(variable = fct_reorder(variable, desc(dropout_loss))) # use the DALEX explainer to get variable important

varimp <- ggplot(var_imp, aes(x = dropout_loss, y = variable)) + # plot variable importance
  geom_boxplot() +
  theme_bw() +
  labs(x = "Drop-out loss", y = "") +
  scale_y_discrete(name = "", labels = c("Baseline", "Geographic Range", "Length", "Carrying Capacity", "Trophic Level", "Movement Keyword", "Full Model", "Growth Rate")) +
  theme(text = element_text(size = 10))
```

```{r}
varimp
```

Drop-out loss is the RMSE value for a model that includes on the variable of interest.

```{r, include=FALSE}
metric_plot <- scatter_plot + varimp + plot_annotation(tag_levels = "a")

# ggsave(metric_plot, file = paste0("fig5.pdf"), path = here::here("results"), width = 10, height = 5)
```

#### Distribution of real versus predicted home ranges for full data set

```{r, include = FALSE}
prediction_p <- ggplot(prediction, aes(predicted_homerange)) + # distribution of predicted values
  geom_histogram(bins = 30) +
  theme_bw() +
  labs(
    x = "Predicted Home Range (km^2)",
    y = "Count",
    title = "Predicted Home Ranges"
  )

real_p <- ggplot(rf_data, aes(hr)) + # distribution of observed values
  geom_histogram(bins = 30) +
  theme_bw() +
  labs(
    x = "Observed Home Range (km^2)",
    y = "Count",
    title = "Observed Home Ranges"
  )

dist_plot <- real_p / prediction_p

# ggsave(dist_plot, file = paste0("distribution_observed_vs_predicted.pdf"), path = here::here("figs"))
```

```{r}
dist_plot
```

Predicted values match the distribution of the actual values well.

#### Orders of Magnitude

The realistic main goal of this model was to predict home range values within the correct order of magnitude. 

```{r, include= FALSE}
rf_result <- test_predict %>%
  mutate(magitude_p = floor(log10(pred))) %>% # calculate the order of magnitude of the predictions
  mutate(magitude_hr = floor(log10(hr))) %>% # calculate the order of magnitude of the home ranges
  mutate(mag_diff = abs(magitude_p - magitude_hr)) # calculate difference in order of magnitude between prediction and home range

magnitude <- rf_result %>%
  group_by(mag_diff) %>% # group by the calculated difference in magnitude
  summarise(count = n()) %>% # get number of predictions in each category
  mutate(percent = count / nrow(test) * 100) # get percentage
```

```{r, include=FALSE}
mag_table <- magnitude %>%
  kbl(digits = 2, col.names = c("Order of Magnitude", "Count", "Percent")) %>%
  kable_minimal()
```

```{r}
mag_table
```

Most values in the testing set were predicted within 1 order of magnitude of the correct value, indicating the model is useful at predicting on the correct order of magnitude.

```{r, include=FALSE}
# write_csv(magnitude, here::here("results", "magnitude_difference_summary.csv"))
```

```{r, include=FALSE}
mean(rf_result$mag_diff) # mean diff in magnitude
median(rf_result$mag_diff) # median diff in magnitude
```
